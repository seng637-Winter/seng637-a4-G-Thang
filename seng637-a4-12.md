**SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group \#: 12     |     |
| -------------- | --- |
| Student Names: |  Andrew Lee   |
|                |  Dillon Pullano   |
|                |  Jonathan Lui   |
|                |  Gi-E Thang   |
|                |  Yaad Sra   |

# Introduction

This report details the findings of our exploration into mutation testing and web application testing. The objective of this assignment was to gain practical experience in assessing the effectiveness of test suites and automating tests for web interfaces.

Part 1 delves into mutation testing. We employed Pitest to inject faults (mutations) into the code of the Range and DataUtilities classes from a previous assignment. Through analysis of the generated mutants and the effectiveness of our original test suites in detecting them, we gained valuable insights into the quality of our tests. Additionally, we explored the concept of equivalent mutants and their impact on mutation score accuracy. Finally, we discuss strategies we could have implemented to further enhance the mutation score.

Part 2 focuses on web application testing using Selenium. We selected a website (details to be specified based on chosen website) and designed automated test cases for at least two functionalities using Selenium IDE. Our test cases incorporate verification points to ensure the application behaves as expected.  This section also details the collaborative effort of our team, including task division, challenges encountered, and lessons learned.

Through this assignment, we gained a deeper understanding of mutation testing as a technique to evaluate test suite effectiveness and the power of Selenium for automating web application tests.

# Analysis of 10 Mutants of the Range class 

The following section discusses 10 mutants picked at semi-random. Our intention was to select a variation of tests associated with different types of mutations and whether they were killed or had survived:


## Mutant #1: (Code Line: 90, Mutation Line: 1, Location: <Init>, Killed by: org.jfree.data.test.RangeTest)

#### Type: changed conditional boundary → KILLED
#### Actual Code: if (lower > upper) 

#### Explanation:

This type means that Pitest tried to change the code to have a different conditional boundary. Two possible examples of this modification could be “lower >= upper” or “lower > upper + 1”. Since the test was KILLED, it shows that the org.jfree.data.test.RangeTest class was able to detect the change.


## Mutant #2: ( Code Line: 95, Mutation Line: 3, Location: <init>, Killed By: None)

#### Type: Incremented (a++) double local variable number 1 → SURVIVED
#### Actual Code: this.lower = lower;

#### Explanation:
This means Pitest tried to change the code to increment (add 1) to a double local variable named "lower" using the post-increment operator (a++).
The original code and the mutated code (where "lower" is incremented by 1) produced the same output for all test cases. This is why the mutation is reported as "SURVIVED". The mutation increments "lower" by 1 using a post-increment operator (a++). This means the value of "lower" is first assigned from the argument passed to the constructor, and then incremented by 1.


## Mutant #3: ( Code Line: 95, Mutation Line: 4, Location: <Init>, Killed by: None )

#### Type: Decremented (a--) double local variable number 1 → SURVIVED
#### Actual Code: this.lower = lower;

#### Explanation:
This means Pitest tried to change the code to decrement (subtract 1) from a double local variable named "lower" using the pre-decrement operator (a--).
The mutation decrements "lower" by 1 using a pre-decrement operator (a--). This means the value of "lower" is first decremented by 1 and then assigned from the argument passed to the constructor.


## Mutant #4: ( Code Line: 123, Mutation Line: 9, Location: getLength, Killed by: org.jfree.data.test.RangeTest)

#### Type: Replaced double subtraction with multiplication → KILLED
#### Actual Code: return this.upper - this.lower;

#### Explanation: 
Pitest changed the code in the getLength method to use multiplication (*) instead of subtraction (-) between this.upper and this.lower. The original code likely calculated the length (extent) by subtracting the lower bound (this.lower) from the upper bound (this.upper). Replacing this with multiplication would produce incorrect results.


## Mutant #5: ( Code Line: 123, Mutation Line: 6, Location: getLength, Killed by: org.jfree.data.test.RangeTest  )

#### Type: Replaced double operation with first member → KILLED
#### Actual Code: return this.upper - this.lower;

#### Explanation: 
Pitest replaced the entire double operation (this.upper - this.lower) with just its first member (this.upper). In essence, the mutated code returns this.upper directly, discarding the subtraction. The org.jfree.data.test.RangeTest likely contains test cases that expect getLength to return the correct length/extent.
The mutation significantly alters the method's output, causing those test cases to fail, hence marking the mutation as "KILLED."


## Mutant #6: (Code Line: 158, Mutation Line: 1, Location: intersects, Killed by: org.jfree.data.test.RangeTest)

#### Type: replaced boolean return with false for org/jfree/data/Range::intersects → KILLED
#### Actual Code: return (b1 > this.lower);

#### Explanation:
Pitest modified this code to make it so that the conditional statement “(b1 > this.lower)” always returns a false boolean, even when the intersect should be true. Since the test was KILLED, it shows that the org.jfree.data.test.RangeTest class was able to detect the change with one of our test functions.


## Mutant #7: ( Code Line: 144 , Mutation Line: 28, Location: Contains, Killed by: org.jfree.data.test.RangeTest )

#### Type: Less than to less or equal → KILLED
#### Actual Code: return (value >= this.lower && value <= this.upper);

#### Explanation: 
Pitest changed the comparison operator in the Contains method from < (less than) to <= (less than or equal to). The original code likely checked if a value (value) was strictly greater than or equal to the lower bound (this.lower) and strictly less than or equal to the upper bound (this.upper) to determine if it was "contained" within the range.

## Mutant #8: ( Code Line: 333, Mutation Line: 13, Location: expand, Killed by: None)

#### Type: Incremented (a++) double local variable number 9 → SURVIVED
#### Actual Code: if (lower > upper) 

#### Explanation:
Pitest modified this code to make it so that the conditional statement “if (lower > upper)” is modified so that local variable 9 (likely to be either lower or upper) is incremented up be 1 before the line is executed. Since the test SURVIVED, it shows that the org.jfree.data.test.RangeTest class was not able to detect the change with one of our test functions.


## Mutant #9: ( Code Line: 387, Mutation Line: 2, Location: shiftWithNoZeroCrossing, Killed by: None)

#### Type: Substituted 0.0 with 1.0 → NO_COVERAGE
#### Actual Code: if (value > 0.0)

#### Explanation:
Pitest modified this code to make it so that the conditional statement “if (value > 0.0)” is modified to “if (value > 1.0). Since the test has NO_COVERAGE, it shows that the org.jfree.data.test.RangeTest class does not cover this function. This makes sense as our RangeTest class did not include tests for the shiftWithNoZeroCrossing Range function.


## Mutant #10: ( Code Line: 475 , Mutation Line: 13, Location: toString, Killed by:  None)

#### Type: Negated double field lower → SURVIVED
#### Actual Code: return ("Range[" + this.lower + "," + this.upper + "]");

#### Explanation: 
​​Pitest negated the value of the this.lower double field within the toString method on line 475. This likely means it changed the sign from positive to negative or vice versa. The original code and the mutated code (where this.lower is negated) produced the same output for all test cases. This is why the mutation is reported as "SURVIVED."


# Report all the statistics and the mutation score for each test class

# Analysis drawn on the effectiveness of each of the test classes

# A discussion on the effect of equivalent mutants on mutation score accuracy

Equivalent Mutants Effects on Mutation Score Accuracy:
Equivalent mutants are defined as being syntactically different while producing the same semantic results. This can be simplified to say that the equivalent mutants look different but behave the same way or produce the same results. Due to this, having many equivalent mutants can give a false sense of how well a test suite performed for mutation testing. 

Let's consider an example where one line of code has 100 equivalent mutants that all pass and there are a total of 500 mutants for the whole evaluation. The other 400 mutants are split over another 20 lines of code and are distributed more or less equally with 50% pass. The accuracy score produced while omitting these equivalent mutants should be close to 50%. However, when they are not omitted, the accuracy score is closer to 60% coverage. For this reason it is important to be able to consider equivalent mutants and look at other methods of coverage.

Automatically Detecting Equivalent Mutants Discussion:
In order to automate the detection of equivalent mutants, we would first need to split up mutations based on the line of code that they are mutated from. We would then need to identify whether or not the change in code was syntactically different. This would be relatively easy by just comparing the lines. If they are different in appearance then they are syntactically different. 

The last (and most difficult) step would be to determine if the modified lines of code were semantically different. This last part is tricky because we would need to have an interpreter program that could extract meaning out of the lines of code and compare them. It might be easier to write a program for more simple cases such as the one from the course notes where (i<10) and (i!=10) are equivalent, but it would be more complex to evaluate semantics on more complex lines of code. 


# A discussion of what could have been done to improve the mutation score of the test suites

To improve the mutation score of the test suites, we focused on enhancing test coverage and code path effectiveness. Our process focused on refining existing tests to increase the mutation coverages rather than introduce a larger number of tests. The overall process is as follows:

We first identified the lines of code that were producing the most amount of mutations that had survived (i.e. low coverage areas). We figured that by starting with the low hanging fruit, we could establish what was occurring in that test and then adapt the found solutions into our other lines of code that were producing similar results from similar mutations.

When we were first playing around with these, we tried to add additional assertEquals conditions to catch more of the edge cases. However, we noticed that for each line we added, that was just another set of mutations that would produce surviving mutations as well. Therefore, the problem had to do with a lack of imposed data restrictions on how these assertEquals were being executed, and not necessarily how they covered boundary and edge cases. 

We found that the most effective method for our tests to identify previously not seen mutations was to add “final” to variables that were not to be modified after they were created. We also made sure that the compared values were cast to the same primitive “Double” so that when passed into the AssertEquals function, we did not have to specify a tolerance value.

Using the above mentioned processes, we iteratively reviewed and updated our test suites until we had increased mutation coverages past the 10% gain threshold for both the DataUtilitiesTest and RangeTest test suites.


# Why do we need mutation testing? Advantages and disadvantages of mutation testing

We need mutation testing because it systematically assesses the quality of our test suite by introducing small changes (mutations) to the codebase and checking if the tests can detect these alterations. It helps improve test case quality, identifies defects missed by other testing methods, and aids in prioritising testing efforts.

Advantages:

Evaluates test suite effectiveness.
Improves test case quality.
Identifies defects missed by other methods.
Offers guidance for test case enhancement.
Helps prioritize testing efforts.

Disadvantages:

Computational complexity.
High false positives.
Manual result analysis required.
Dependency on test suite quality.
Limited tool support.


# Explain your SELENUIM test case design process

# Explain the use of assertions and checkpoints

# how did you test each functionaity with different test data

# How the team work/effort was divided and managed

The team's work was divided and managed collaboratively:

- **Task Division:** We divided the tasks based on individual strengths and expertise. Jonathan focused on designing and implementing Selenium test scripts for the Home Depot website, while other team members contributed to test scenario identification, data preparation, and script review.

- **Regular Communication:** We maintained regular communication throughout the assignment to share progress updates, discuss challenges, and coordinate efforts effectively. This ensured that everyone stayed aligned with the project goals and timelines.

- **Collaborative Review:** We conducted peer reviews of the test scripts to ensure quality and consistency. Each team member provided feedback and suggestions for improvements, fostering a collaborative environment and enhancing the overall quality of the deliverables.


# Difficulties encountered, challenges overcome, and lessons learned

During the assignment, we encountered several difficulties and challenges:

- **Tool Familiarity:** Some team members faced initial challenges in getting acquainted with Selenium IDE and its features. However, through self-learning and exploration, we quickly gained proficiency and were able to effectively utilize the tool for test automation.

- **Test Data Management:** Managing and maintaining different sets of test data for various scenarios proved to be challenging at times. To overcome this, we established a structured approach to organize and document test data, ensuring clarity and consistency across test cases.

- **Debugging and Troubleshooting:** Debugging test scripts and identifying errors or failures required careful attention and troubleshooting skills. We learned to leverage Selenium's debugging capabilities and log analysis techniques to identify and resolve issues efficiently.

Overall, these challenges provided valuable learning experiences, and we emerged with enhanced skills in test automation and collaboration.


# Comments/feedback on the assignment itself

The assignment provided an excellent opportunity to gain practical experience in mutation testing and web application testing using Pitest and Selenium, respectively. The hands-on exercises allowed us to apply theoretical concepts learned in class and deepen our understanding of software testing principles and techniques.

However, we found that the assignment instructions could be more detailed and structured, especially regarding the setup and configuration of testing tools. Providing clearer guidelines and step-by-step instructions would help streamline the implementation process and reduce confusion among students.

Additionally, incorporating more real-world scenarios or case studies into the assignment tasks could further enrich the learning experience and prepare students for practical challenges in software testing.

Overall, we appreciate the opportunity to engage in this assignment and look forward to applying our newfound knowledge and skills in future projects.
